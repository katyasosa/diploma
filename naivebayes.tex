\subsection{Наивный байесовский классификатор}
\subsubsection{Описание классификатора}
Сперва рассмотрим простой случай преобразования твита в числовой вектор. Для этого построим словарь
на основе обучающих данных: каждое слово -- это отдельный признак (или фича от английского feature),
который измеряется в конкретном сообщении. Пусть в тренировочной выборке было $D$ уникальных слов,
тогда числовой вектор, соответствующий твиту, выглядит следующим образом:
$\mathbf{x} = \{x_1,\ldots,x_D \}$. Здесь каждая компонента $x_j$ показывает, как представлено
$j$-ое слово в сообщении $x$. Будем считать, что данные распределены по закону Бернулли, тогда
представленность слова определяется просто его наличием или отсутствием, то есть $x_j = 1$, если
$j$-ое слово встретилось в сообщении, и $0$ в противном случае. Задача -- сопоставить каждому такому
вектору наиболее правдоподобную метку класса, которую будем обозначать $y$. Множество всех классов
обозначим $\mathcal{C}$. Тем самым, для каждого класса $c \in \mathcal{C}$ необходимо посчитать
условную вероятность $p(\mathbf{x}|y=c)$. Наивный байесовский классификатор действительно наивен в
том смысле, что он предполагает независимость признаков. Это позволяет считать искомую вероятность
как произведение вероятностей для каждой фичи отдельно:

\begin{equation}
  p(\mathbf{x}|y=c,\mathbf{\theta}) = \prod_{j=1}^Dp(x_j|y=c,\mathbf{\theta}_{jc})
  \label{eq:nbprob}
\end{equation}

Здесь $\mathbf{\theta}$~-- это матрица-параметр распределения Бернулли, который ищется на этапе
обучения классификатора, то есть по известным парам $\mathbf{x}, y$. На самом деле элемент этой
матрицы  $\mathbf{\theta}_{jc}$~-- это вероятность того, что $j$-ое слово встретится в примерах из класса $c$.


\subsubsection{Обучение и предсказание}
На этапе обучения классификтора нужно найти, как уже было сказано, параметры распределения,
моделирующего данные, и априорные вероятности классов $\mathbf{\pi}$. Сперва обозначим тренировочные
данные $\mathcal{D}$, тогда $\mathbf{x}_i \in \mathcal{D}$~-- это пример из обучающей выборки, а
$x_{i1},\ldots,x_{iD}$~-- количественные характеристики признаков для примера $\mathbf{x}$, и $y_i$~-- метка класса для него.

Посмотрим на $i$-ый пример, тогда вычисление вероятности пары $\mathbf{x}_i, y_i$ при условии
параметров $\mathbf{\theta}$ происходит по формуле:

\begin{equation}
  p(\mathbf{x}_i, y_i|\mathbf{\theta})
  = p(y_i|\mathbf{\pi})\prod_{j=1}^Dp(x_{ij}|\mathbf{\theta}_j)
  = \prod_{c\in\mathcal{C}}\pi_c^{\mathbb{I}(y_i=c)}
    \prod_{j=1}^D\prod_{c\in\mathcal{C}}p(x_{ij}|\mathbf{\theta}_{jc})^{\mathbb{I}(y_i=c)}
  \label{eq:mle}
\end{equation}

Переходя ко всему набору данных получается, что нужно максимизировать следующую величину

\begin{equation}
  p(\mathcal{D}|\mathbf{\theta})
  = \prod_{c\in\mathcal{C}}\pi_c
    \prod_{j=1}^D\prod_{c\in\mathcal{C}}\prod_{i:y_i=c}p(x_{ij}|\mathbf{\theta}_{jc})
  \label{eq:mleD}
\end{equation}

и найти сответствующие этому максимуму параметры $\mathbf{\pi}$ и $\mathbf{\theta}_{ij}$. Произведя
все необхоимые вычисления получим, что
\begin{equation}
  \hat{\pi}_c = \frac{N_c}{N}
  \label{eq:pinb}
\end{equation}
\begin{equation}
  \hat{\theta}_{jc}= \frac{N_{jc}}{N_c}
  \label{eq:thetanb}
\end{equation}
В этих формулах $N$~-- количество примеров в обучающей выборке, $N_c$~--количество элементов в
классе $c$,
$N_{jc}$~-- количество раз, которое $j$-ое слово встретилось в примерах из класса $c$. Обучение
модели происходит за $O(ND)$ шагов.

Для предсказания класса $y$ для нового примера $\mathbf{x}$ необходимо максимизовать следующее
выражение по всем классам $c \in \mathcal{C}$

\begin{equation}
  \hat{\pi}_c\prod_{j=1}^D(\hat{\theta}_{jc})^{\mathbb{I}(x_j=1)}(1-\hat{\theta}_{jc})^{\mathbb{I}(x_j=0)}
  \label{eq:predict}
\end{equation}

Класс, для которого оно получилось максимальным, и считается наиболее правдоподобным для данного
примера.

\subsubsection{Проблемы подхода}

Основная проблема описанного подхода заключается в том, что не обязательно все слова (признаки)
встретятся во всех классах. Например, если $j$-ое слово не представлено в классе $c$, то $N_{jc}=0$
и вероятность того, что рассматриваемое сообщение попадёт в класс $c$ тоже станет нулевой. От этой
проблемы может избавить переход к байесовскому подходу, когда параметры становятся случайными
величинами со своими распределениями, и такая ситуация невозможно. Для устранения этой проблемы
используется эвристика, когда и к числителю, и к знаменателю прибавляется по 1.

Другая проблема, с которой необходимо бороться,~-- это невозможность расширения словаря: если слова
не было в обучающей выборке, то классификатор снова сталкивается с нулевыми вероятностями, но теперь
для всех классов.

\subsection{Вероятностная модель нового метода}
\subsubsection{Переход к байесовскому подходу}\label{bnb}
Чтобы избавиться от проблемы с нулевыми вероятностями ``честным'' способом, перейдём к байесовскому
наивному байесовскому классифиактору. В этом случае параметры вместо точечных величин представляются
случаными величинами с априорными распределениями. В модели классификатора параметры -- это
вероятности, то есть их значения ограничены на отрезке $[0,1]$. Так распределение для каждого из
параметров должно задавать случайную величину, ограниченную на отрезке. Для моделирования параметра
$\mathbf{\pi}$ будем использовать распределение Дирихле $\operatorname{Dir}(\mathbf{\alpha_0})$, которое
устанавливает закон распределения многомерной случайной величины, где все компоненты~-- величины из
отрезка $[0,1]$ и суммируются в $1$. Считаем также, что каждый из параметров $\theta_{jc}$ имеет
Бета-распределение на отрезке $[0,1]$ $\operatorname{Beta}(\beta_0,\beta_1)$. Все параметры
считаются независимыми, поэтому априорная вероятность параметров считается по формуле
\begin{equation}
  p(\mathbf{\theta}) = p(\mathbf{\pi})\prod_{j=1}^D\prod_{c\in\mathcal{C}}p(\theta_{jc})
  \label{eq:apr}
\end{equation}
Задача теперь получить апостериорную вероятность на основании данных $\mathcal{D}$ из обучающей
выборки и выяснить, попадёт ли она в тот же класс, что и априорная, то есть получится ли для неё
выражение вида
$\operatorname{Dir}(\ldots)\cdot\operatorname{Beta}(\ldots)\cdot\ldots\cdot\operatorname{Beta}(\ldots)$.
По теореме Байеса искомую вероятность можно посчитать по следующей формуле
\begin{equation}
  p(\mathbf{\theta}|\mathcal{D}) =
  \frac{p(\mathbf{\theta})\cdot p(\mathcal{D}|\mathbf{\theta})}{p(\mathcal{D})}
  \label{eq:apo}
\end{equation}
Знаменатель выражения не зависит от параметров, поэтому рассмотрим отдельно числитель. Если
расписать плотности распределений и сгруппировать полученные призведения, получится вывод,
сокращённая версися которого приведена ниже. $M$ здесь некоторая постоянная.
\begin{align}
  p(\mathbf{\theta}) \cdot p(\mathcal{D}|\mathbf{\theta})
  & =  M\cdot\prod_{c\in\mathcal{C}}\pi_c^{\alpha_0c-1}\cdot
  \prod_{j=1}^D\prod_{c\in\mathcal{C}}\theta_{jc}^{\beta_0}(1-\theta_{jc})^{\beta_1}\cdot
  \prod_{\mathbf{x}\in\mathcal{D}}\prod_{c\in\mathcal{C}}\pi_c\prod_{j=1}^D\theta_{jc}^{x_{jc}} \label{eq:plo1}\\
  & = M\cdot\prod_{c\in\mathcal{C}}\pi_c^{\alpha_{0c}+N_c}\cdot
  \prod_{c\in\mathcal{C}}\prod_{j=1}^D\theta_{jc}^{\beta_0+N_c-N_{jc}}(1-\theta_{jc})^{\beta_1+N_{jc}}
  \label{eq:plo2}
\end{align}
В выражении \label{eq:plo2} записано ни что иное как произведение плотностей распределения Дирихле и
Бета-распределений. Так вероятность $p(\mathbf{\theta}|\mathcal{D})$ осталась в том же классе, что и
$p(\mathbf{\theta})$.

\begin{align}
  p(\mathbf{\pi}|\mathcal{D}) &= \operatorname{Dir}(N_1+\alpha_{01},\ldots,N_c+\alpha_{0c},\ldots)\label{eq:pi}\\
  p(\theta_{jc}|\mathcal{D}) &= \operatorname{Beta}(N_c-N_{jc} + \beta_0, N_{jc}+\beta_1) \label{eq:thetajc}
\end{align}
Обозначим получившиеся параметры как $\alpha_0^*$, $\beta_0^*$ и $\beta_1*$ соответственно.

Остаётся понять, как при помощи полученных данных можно предсказать класс для новых
примеров. Рассмотрим пример $\mathbf{x}$. Хотим найти для него значение метки $y$. Для этого нужно
максимизировать $p(y=c|\mathbf{x},\mathcal{D})$ по всем $c\in\mathcal{C}$.

\begin{equation}
  p(y=c|\mathbf{x},\mathcal{D}) =
  \frac{p(y=c|\mathcal{D})\cdot p(\mathbf{x}|y=c,\mathcal{D})}{p(\mathbf{x}|\mathcal{D})}
  \label{eq:bnbpred}
\end{equation}

Снова будем смотреть только на числитель. Первый его множитель на самом деле является математическим
ожиданием $\pi_c$ по $\mathbf{\pi}$ c плотностью распределения Дирихле. Краткая версия вывода
приведена ниже, в \ref{eq:mathpi1}, \ref{eq:mathpi2} и \ref{eq:mathpi3}. Здесь для
преобразования \label{eq:mathpi1} используется техника маргинализации, а переход \ref{eq:mathpi3}
следует из определения математического ожидания.
\begin{align}
  p(y=c|\mathcal{D})&=\int_{\mathbf{\pi}\in\Pi}p(y=c,\mathbf{\pi}|\mathcal{D})d\mathbf{\pi}\label{eq:mathpi1}\\
  &= \int_{\mathbf{\pi}\in\Pi}p(\mathbf{\pi}|\mathcal{D})p(y=c|\mathcal{D},\mathbf{\pi})d\mathbf{\pi}\label{eq:mathpi2}\\
  &= \underset{\mathbf{\pi}\sim\operatorname{Dir(\alpha_0^*)}}{\mathbb{E}}\pi_c
  \label{eq:mathpi3}
\end{align}

Аналогично первому разбираем второй множитель. Сперва снова воспользуемся техникой маргинализации
для записи \ref{eq:maththeta1}. Теперь раскрываем вероятность  в произведение вероятностей
-- так можно делать, потому что величины $x_i$ и $\theta_{*c}$ независимы, и получаем формулу
\ref{eq:maththeta2}. Далее переписываем интеграл в обозначениях математического ожидания и, согласно
посчитанному ранее, приходим к тому, что благодаря независимости параметров получили произведение
мат. ожиданий случайных величин $\theta_{jc}$ с плотностью Бета-распределения.

\begin{align}
  p(\mathbf{x}|y=c,\mathcal{D}) &=\int_{\theta\in\Theta}p(\mathbf{x},\theta_{*c}|y=c,\mathcal{D})d\theta_{*c}\label{eq:maththeta1}\\
  &=\int_{\theta\in\Theta}p(\mathbf{x}|y=c,\mathcal{D},\theta_{*c})p(\theta_{*c}|y=c,\mathcal{D})d\theta_{*c}\label{eq:maththeta2}\\
  &=\underset{p(\theta_{*c}|y=c,\mathcal{D})} {\mathbb{E}}p(\mathbf{x}|y=c,\mathcal{D},\theta_{*c})\label{eq:maththeta2}\\
  &=\prod_{j=1}^D\underset{\theta_{*c}\sim\operatorname{Beta}(\beta_0^*,\beta_1^*)}{\mathbb{E}}\theta_{jc}^{x_{ij}}
\end{align}

Так как параметры вычислены на этапе обучения, а формула математических ожиданий для
соответствующих распределений извесна, остаётся только подставить их в выражение \ref{eq:bnbpred},
максимум которого мы ищем.

\subsubsection{Использование n-грамм для измерения признаков}\label{ngram}
Альтернативный взгляд на подсчёт вероятностей сообщений -- это разбиение сообщения на $n$-граммы, то
есть $n$-буквенные сочетания. Такое изменение позолит искусственно ослабить требование о
независимости признаков. Допустим, что исходное сообщение разбивается на 3-граммы. 3 здесь выбрано,
так как вычислительно количество всех возможных 3-грамм ещё не так велико, но уже информативно.
Переобозначим $x_i$, теперь они представляют 3 граммы. Пусть 3-грамм в сообщении всего $L$, тогда
вектор $\mathbf{x} = {x_1, \ldots, x_L}$ описывает представленность 3-грамм в сообщении.
Вероятность того, что сообщение попадёт в класс $c$, в этом случае подсчитывается по следующей формуле

\begin{equation}
  p(\mathbf{x}, y=c) = \prod_{j=2}^Lp(x_j|x_{j-1},y=c)
\end{equation}

Так модель классификатора немного изменится, и новыми параметрами станут как раз
$p(x_j|x_{j-1},y=c)$, которые и будет искать алгоритм на этапе обучения.

Перейдём к предыдущим обозначениям и выясним, как изменятся формулы подсчёта параметров и максимума
правдоподобия. Вектор $\mathbf{x}$ снова показывает представленность каждой триграммы из обучающей
выборке в рассмтариваемом примере: $1$ стоит на месте присутствующей триграммы и $0$ на месте
отсутствующей. Введём также функцию, которая поможет понимать последовательность триграмм в примере:
$n(x_{j})$ показывает, какой по счёту встретилась $j$-ая триграмма в примере. Если не встретилась,
то пускай значение этой функции будет $-\inf$. Для простоты считаем,
что каждая триграмма встретилась только один раз. В формуле \ref{eq:pi} пересчёта параметра $\mathbf{\pi}$ ничего не изменится, так как
на априорные вероятности класса изменение не повлияет. Переобозначим $D$~-- это теперь количество всех
3-грамм, встретившихся в обучающей выборке. Дополнительно обозначим $N_{jkc}$~--
количество пар из триграммы с номером $k$ и следующей прямо за ней триграммы с номером $j$ в классе $c$.
$\theta_{jkc}$~-- вероятность встретить триграмму с номером $k$ сразу за триграммой с номером $j$ в
классе $c$. Тогда пересчёт параметра $\theta_{jkc}$ будет производиться по формуле, аналогичной
\ref{eq:thetajc}, но с учётом условности

\begin{equation}
  p(\theta_{jkc}|\mathcal{D}) = \operatorname{Beta}(N_c-N_{jkc}+\beta_0, N_{jkc}+\beta_1)
  \label{eq:thetajkc}
\end{equation}

Новые параметры опять обозначим $\beta_0^*$ и $\beta_1^*$ для удобства. В формуле \ref{eq:bnbpred} снова будем смотреть только на числитель. Первый множитель не изменится,
а второй будет вычисляться так:

\begin{equation}
   p(\mathbf{x}|y=c,\mathcal{D}) =
   \prod_{j=1}^D\prod_{k=1}^D\underset{\theta_{**c}\sim\operatorname{Beta}(\beta_0^*,\beta_1^*)}{\mathbb{E}}\theta_{jkc}^{\mathbb{I}(n(x_{j})=n(x_{k})+1)}
   \label{eq:maththetagr}
\end{equation}

Причём получится, что каждое $\theta_{jkc}$ представлена не больше, чем один раз, так как в примере
за триграммой может следовать только одна другая.
