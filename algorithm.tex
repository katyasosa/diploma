\subsection{Онтологии для замены неизвестных слов}\label{ontology}
Способ содержательного устранения проблемы с неизвестностью слов в анализируемых примерах,
предлагаемый в работе -- это использование онтологий. Онтологией называется некоторая схема области
знаний, обычно она представляет собой направленный ациклический граф, вершины которого --
понятия. Чем выше в графе находится вершина, тем шире соответствующее ей понятие. Таким образом,
если в сообщении встретилось слово, которое классификатору не известно, его можно заменить на более
общее понятие в соответствии с онтологией.

База знаний Википедии\footnote{https://www.wikipedia.org/} составляется пользователями и на данный
момент содержит 4,515,000 статей только на английском языке. Структура организации статей в
Википедии похожа на то, что мы ищем, -- это граф категорий.
На основе категорий википедии уже составлена онтология проектом DBpedia\cite{dbpedia-swj} -- она и
используется в работе.

Данные выкачиваются в формате
Turtle\footnote{http://www.w3.org/TR/turtle/}. У нас есть два файла: первый -- отображение статьей в
подмножество категорий, второй -- лес всех категорий. Граф категорий сильно несвязный, большинство
категорий не участвуют ни в какой иерархии. Согласно этим особенностям нужно решить задачу хранения
онтологии и поиска нужной категории.

В первую очередь, стоит сказать, что хранится отображение категорий в номера вершин графе, так как
хранить целые строчки для такого количества данных неэкономно. Эти отображения кладутся в
бор\footnote{Marisa trie: https://code.google.com/p/marisa-trie/}, в
соседнем боре запоминаются отображения статей в категории.
Когда появляется неизвестное слово, ищется статья, где искомое
слово -- префикс, так как названия статей не всегда из одного слова. Выбираем ту категорию из всех, к которым относится данная статья, что в иерархии находится ниже всего. При равенстве уровня выбираем любую.

\subsection{Реализация}
Описанные идеи в ходе данной работы были объединены в алгоритм классификации и реализованы на языке
Python и его расширении Cython. Ниже
приведена последовательность действий со ссылками на разделы в тексте, где про эти действия
рассказывается более подробно. Некоторые моменты не были разъяснены ранее, поэтому им уделяется чуть
больше внимания. Отдельно рассмотрим три части: подготовка данных, обучение классификатора,
предсказание класса.

\textbf{Подготовка данных} (некоторые пункты не относятся к данным из обучающей выборки, в этом
случае в конце пункта стоит ``*''):
\begin{enumerate}
  \item замена сущностей html на слово ``URL'', упоминаний пользователя на ``USER'' и чисел на ``42'';
  \item перевод сообщения в нижний регистр;
  \item замена смайлов, хештегов и некоторых знаков препинания на ``+'' или ``-'' (раздел \ref{spec});
  \item замена долгого повторения гласных на сочетание из двух букв (раздел \ref{spec});
  \item разбивка на слова с учётом знаков препинания при помощи Penn Treebank Tokenizer из NLTK\cite{bird2006nltk};
  \item замена скоращений на их расшифровки (раздел \ref{spec});
  \item замена неизвестных классификатору слов на обобщения каждого из них (раздел \ref{ontology});*
  \item приведение всех слов к начальной форме при помощи алгоритма Snowball
  Stemmer\cite{porter2001snowball}
  \item удаление артиклей, предлогов и союзов из сообщения.
\end{enumerate}

\textbf{Обучение классификатора:}
\begin{enumerate}
\item подготовка данных и сохранение множества известных слов;
\item преобразование полученных строк на перекрывающиеся пары триграмм\footnote{Из предложения
    ``Мама мыла раму.'' получатся следующие пары триграмм: ``Мам|а м'', ``а м|ыла'', ``ыла| ра'', ``
    ра|му.''}, теперь одна пара триграмм -- это признак, по которому измеряются сообщения;
\item каждый пример из обучающей выборки преобразуется в числовой вектор: на месте соответствующей
  пары триграмм из набора ставится $1$, если она есть в примере, и $0$, если нет;
\item поиск апостериорных распределений параметров согласно формулам \ref{eq:pi} и \ref{eq:thetajkc}.
\end{enumerate}

\begin{samepage}
  \textbf{Предсказание класса для нового примера:}
  \begin{enumerate}
  \item подготовка примера согласно описанному выше в ``Подготовка данных'';
  \item преобразование полученной строки на перекрывающиеся пары триграмм;
  \item преобразование полученного набора в числовой вектор: на месте соответствующей
    пары триграмм из набора ставится $1$, если она есть в примере, и $0$, если нет;
  \item вычисление вероятности примера оказаться в каждом из классов (``1'' или ``-1'') согласно выражению
    \ref{eq:bnbpred} и вычислению множителей из неё по \ref{eq:mathpi3} и \ref{eq:maththetagr}
  \item выбор класса, который дал наибольшую вероятность попадания в него.
  \end{enumerate}
\end{samepage}
